#!/usr/bin/env python3
"""
Script ƒë·ªÉ crawl n·ªôi dung t·ª´ c√°c trang tarot card.
L·∫•y d·ªØ li·ªáu t·ª´ div.content__body v√† format theo chu·∫©n JSON.
"""

import json
import re
import time
from pathlib import Path
from typing import Dict, List

import requests
from bs4 import BeautifulSoup


def sanitize_html(html: str) -> str:
    """
    Sanitize HTML ƒë·ªÉ c√≥ th·ªÉ l∆∞u v√†o database d·∫°ng JSONB.
    Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát c√≥ th·ªÉ g√¢y v·∫•n ƒë·ªÅ v·ªõi JSON.
    """
    # Lo·∫°i b·ªè c√°c k√Ω t·ª± control characters (ngo·∫°i tr·ª´ \n, \r, \t)
    html = re.sub(r'[\x00-\x08\x0B-\x0C\x0E-\x1F]', '', html)
    # Normalize whitespace
    html = re.sub(r'\s+', ' ', html)
    html = html.strip()
    return html


def should_skip_li(li_tag) -> bool:
    """
    Ki·ªÉm tra xem th·∫ª <li> c√≥ ch·ª©a th·∫ª <a> b√™n trong kh√¥ng.
    N·∫øu c√≥ th√¨ b·ªè qua.
    """
    return li_tag.find('a') is not None


def extract_content_blocks(soup: BeautifulSoup) -> List[Dict]:
    """
    Tr√≠ch xu·∫•t c√°c block n·ªôi dung t·ª´ div.content__body.
    
    Args:
        soup: BeautifulSoup object c·ªßa trang HTML
        
    Returns:
        List c√°c block v·ªõi format: {index, type, html}
    """
    content_body = soup.find('div', class_='content__body')
    
    if not content_body:
        return []
    
    blocks = []
    index = 1
    processed_elements = set()  # Track c√°c ph·∫ßn t·ª≠ ƒë√£ x·ª≠ l√Ω
    
    # H√†m helper ƒë·ªÉ ki·ªÉm tra xem m·ªôt ph·∫ßn t·ª≠ c√≥ n·∫±m trong ph·∫ßn t·ª≠ ƒë√£ x·ª≠ l√Ω kh√¥ng
    def is_processed_or_inside_processed(element):
        current = element
        while current:
            if id(current) in processed_elements:
                return True
            current = current.parent
        return False
    
    # Duy·ªát t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠ con c·ªßa content__body theo th·ª© t·ª±
    # S·ª≠ d·ª•ng find_all v·ªõi recursive=True ƒë·ªÉ t√¨m t·∫•t c·∫£, sau ƒë√≥ l·ªçc
    all_elements = content_body.find_all(['p', 'ul', 'li', 'h1', 'h2', 'h3'], recursive=True)
    
    for element in all_elements:
        # B·ªè qua n·∫øu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ho·∫∑c n·∫±m trong ph·∫ßn t·ª≠ ƒë√£ x·ª≠ l√Ω
        if is_processed_or_inside_processed(element):
            continue
        
        tag_name = element.name
        
        # X·ª≠ l√Ω <ul>: Lo·∫°i b·ªè c√°c <li> c√≥ <a> tr∆∞·ªõc khi l∆∞u
        if tag_name == 'ul':
            # T·∫°o b·∫£n sao ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn DOM g·ªëc
            ul_copy = BeautifulSoup(str(element), 'lxml').find('ul')
            if not ul_copy:
                continue
            
            # Lo·∫°i b·ªè c√°c <li> c√≥ ch·ª©a <a>
            li_items = ul_copy.find_all('li', recursive=False)
            for li in li_items:
                if should_skip_li(li):
                    li.decompose()
            
            # Ch·ªâ l∆∞u n·∫øu <ul> c√≤n √≠t nh·∫•t 1 <li> v√† c√≥ n·ªôi dung text
            remaining_lis = ul_copy.find_all('li')
            if remaining_lis and ul_copy.get_text(strip=True):
                html = str(ul_copy)
                html = sanitize_html(html)
                blocks.append({
                    'index': index,
                    'type': 'ul',
                    'html': html
                })
                index += 1
                processed_elements.add(id(element))
        
        # X·ª≠ l√Ω <li>: Ch·ªâ l·∫•y c√°c <li> ƒë·ªôc l·∫≠p (kh√¥ng n·∫±m trong <ul>)
        elif tag_name == 'li':
            # Ki·ªÉm tra xem <li> n√†y c√≥ n·∫±m trong <ul> kh√¥ng
            parent = element.parent
            if parent and parent.name == 'ul':
                # <li> n·∫±m trong <ul> s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω khi x·ª≠ l√Ω <ul>, b·ªè qua
                continue
            
            # <li> ƒë·ªôc l·∫≠p, ki·ªÉm tra xem c√≥ ch·ª©a <a> kh√¥ng
            if should_skip_li(element):
                continue
            
            # L·∫•y HTML c·ªßa <li> ƒë·ªôc l·∫≠p
            html = str(element)
            html = sanitize_html(html)
            
            if element.get_text(strip=True):
                blocks.append({
                    'index': index,
                    'type': 'li',
                    'html': html
                })
                index += 1
                processed_elements.add(id(element))
        
        # X·ª≠ l√Ω c√°c th·∫ª kh√°c: p, h1, h2, h3
        elif tag_name in ['p', 'h1', 'h2', 'h3']:
            # Ki·ªÉm tra xem th·∫ª n√†y c√≥ n·∫±m trong <ul> ho·∫∑c <li> kh√¥ng
            parent = element.parent
            is_inside_list = False
            while parent and parent.name:
                if parent.name in ['ul', 'li']:
                    is_inside_list = True
                    break
                parent = parent.parent
            
            if not is_inside_list:
                # Kh√¥ng n·∫±m trong list, l·∫•y th·∫ª n√†y
                html = str(element)
                html = sanitize_html(html)
                
                if element.get_text(strip=True):
                    blocks.append({
                        'index': index,
                        'type': tag_name,
                        'html': html
                    })
                    index += 1
                    processed_elements.add(id(element))
    
    return blocks


def crawl_card_content(card_url: str, card_id: int) -> Dict:
    """
    Crawl n·ªôi dung t·ª´ m·ªôt URL c·ª• th·ªÉ.
    
    Args:
        card_url: URL c·ªßa trang tarot card
        card_id: ID c·ªßa l√° b√†i
        
    Returns:
        Dict ch·ª©a th√¥ng tin crawl v·ªõi format:
        {
            'url': str,
            'status': str,
            'cardId': int,
            'total_blocks': int,
            'blocks': List[Dict]
        }
    """
    result = {
        'url': card_url,
        'status': 'success',
        'cardId': card_id,
        'total_blocks': 0,
        'blocks': []
    }
    
    try:
        # Th√™m delay ƒë·ªÉ tr√°nh b·ªã block
        time.sleep(1)
        
        # Fetch HTML
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(card_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # Parse HTML
        soup = BeautifulSoup(response.content, 'lxml')
        
        # Extract blocks
        blocks = extract_content_blocks(soup)
        
        result['total_blocks'] = len(blocks)
        result['blocks'] = blocks
        
        if len(blocks) == 0:
            result['status'] = 'warning'
            print(f"‚ö†Ô∏è  Warning: Kh√¥ng t√¨m th·∫•y n·ªôi dung cho card ID {card_id} ({card_url})")
        else:
            print(f"‚úÖ Success: Crawl ƒë∆∞·ª£c {len(blocks)} blocks cho card ID {card_id}")
            
    except requests.exceptions.RequestException as e:
        result['status'] = 'error'
        result['error'] = str(e)
        print(f"‚ùå Error: Kh√¥ng th·ªÉ crawl card ID {card_id} ({card_url}): {e}")
        
    except Exception as e:
        result['status'] = 'error'
        result['error'] = str(e)
        print(f"‚ùå Error: L·ªói kh√¥ng x√°c ƒë·ªãnh khi crawl card ID {card_id}: {e}")
    
    return result


def main():
    """H√†m main ƒë·ªÉ crawl t·∫•t c·∫£ c√°c l√° b√†i."""
    # ƒê∆∞·ªùng d·∫´n ƒë·∫øn file tarot_card.json
    script_dir = Path(__file__).parent
    input_file = script_dir / 'tarot_card.json'
    output_file = script_dir / 'tarot_cards_content.json'
    
    # ƒê·ªçc danh s√°ch c√°c l√° b√†i
    print(f"üìñ ƒêang ƒë·ªçc file {input_file}...")
    with open(input_file, 'r', encoding='utf-8') as f:
        cards = json.load(f)
    
    print(f"üìã T√¨m th·∫•y {len(cards)} l√° b√†i c·∫ßn crawl\n")
    
    # Crawl t·ª´ng l√° b√†i
    results = []
    total_cards = len(cards)
    
    for idx, card in enumerate(cards, 1):
        card_id = card['id']
        card_name = card['card_name']
        card_url = card['card_url']
        
        print(f"[{idx}/{total_cards}] ƒêang crawl: {card_name} (ID: {card_id})...")
        
        result = crawl_card_content(card_url, card_id)
        results.append(result)
        
        # Th√™m delay nh·ªè gi·ªØa c√°c request
        if idx < total_cards:
            time.sleep(0.5)
    
    # L∆∞u k·∫øt qu·∫£
    print(f"\nüíæ ƒêang l∆∞u k·∫øt qu·∫£ v√†o {output_file}...")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # Th·ªëng k√™
    success_count = sum(1 for r in results if r['status'] == 'success' and r['total_blocks'] > 0)
    warning_count = sum(1 for r in results if r['status'] == 'warning')
    error_count = sum(1 for r in results if r['status'] == 'error')
    
    print(f"\nüìä Th·ªëng k√™:")
    print(f"   ‚úÖ Th√†nh c√¥ng: {success_count}/{total_cards}")
    print(f"   ‚ö†Ô∏è  C·∫£nh b√°o (kh√¥ng c√≥ n·ªôi dung): {warning_count}/{total_cards}")
    print(f"   ‚ùå L·ªói: {error_count}/{total_cards}")
    print(f"\n‚ú® Ho√†n th√†nh! K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o {output_file}")


if __name__ == '__main__':
    main()

